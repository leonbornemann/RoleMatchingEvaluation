{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "neural-advance",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Edges - Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "intimate-special",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading austintexas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leon.bornemann/dataset_versioning/finalExperiments/code/RoleMatchingEvaluation/plotting/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3263: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "/home/leon.bornemann/dataset_versioning/finalExperiments/code/RoleMatchingEvaluation/plotting/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3263: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading chicago\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leon.bornemann/dataset_versioning/finalExperiments/code/RoleMatchingEvaluation/plotting/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3263: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading gov.maryland\n",
      "Reading oregon\n",
      "Reading utah\n",
      "Reading education\n",
      "Reading football\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leon.bornemann/dataset_versioning/finalExperiments/code/RoleMatchingEvaluation/plotting/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3263: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "/home/leon.bornemann/dataset_versioning/finalExperiments/code/RoleMatchingEvaluation/plotting/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3263: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading military\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leon.bornemann/dataset_versioning/finalExperiments/code/RoleMatchingEvaluation/plotting/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3263: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading politics\n",
      "Reading tv_and_film\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leon.bornemann/dataset_versioning/finalExperiments/code/RoleMatchingEvaluation/plotting/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3263: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "/home/leon.bornemann/dataset_versioning/finalExperiments/code/RoleMatchingEvaluation/plotting/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3263: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'datasetNamesSocrata' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-1-dad360afe474>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     39\u001B[0m     \u001B[0mreadDF\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mweightedConfig\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0munweightedConfig\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mdatasetName\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     40\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 41\u001B[0;31m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mallDfs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mdatasetNamesSocrata\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"maxRecall\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdtypes\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m: name 'datasetNamesSocrata' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from plotnine import *\n",
    "from plotnine.data import *\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix, precision_recall_curve\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#####################################################################\n",
    "#### REPLACE THIS WITH THE PATH TO YOUR DATA ########################\n",
    "pathToData = '/san2/data/change-exploration/roleMerging/finalExperiments/finalResultFiles/' \n",
    "#####################################################################\n",
    "#####################################################################\n",
    "\n",
    "def prepareDF(df):\n",
    "    df['edgeID'] = df['vertex1ID'] + '_' + df['vertex2ID']\n",
    "    df['edgeIDIsSorted'] = df['vertex1ID'] > df['vertex2ID']\n",
    "    filtered=df[df['evidence']>0]\n",
    "    return filtered[filtered['cliqueSize']>1]\n",
    "\n",
    "def readDF(weightedConfig,unweightedConfig,datasetName):\n",
    "    maxRecallConfig = 'max_recall'\n",
    "    pathMaxRecall=pathToData + datasetName + '/' + maxRecallConfig + '/edges.csv'\n",
    "    pathWeighted=pathToData + datasetName + '/' + weightedConfig +'/edges.csv'\n",
    "    pathUnweighted=pathToData + datasetName + '/' + unweightedConfig +'/edges.csv'\n",
    "    dfEdgesMaxRecall = prepareDF(pd.read_csv(pathMaxRecall))\n",
    "    dfEdgesWeighted = prepareDF(pd.read_csv(pathWeighted))\n",
    "    dfEdgesUnweighted = prepareDF(pd.read_csv(pathUnweighted))\n",
    "    datasetResult = {'maxRecall':dfEdgesMaxRecall,weightedConfig:dfEdgesWeighted,unweightedConfig:dfEdgesUnweighted}\n",
    "    allDfs[datasetName] = datasetResult\n",
    "\n",
    "weightedConfig = \"weighted\"\n",
    "unweightedConfig = \"unweighted\"\n",
    "datasetNames = [\"austintexas\",\"chicago\", \"gov.maryland\",  \"oregon\",  \"utah\", \"education\",\"football\",\"military\", \"politics\", \"tv_and_film\"]\n",
    "\n",
    "allDfs = {}\n",
    "\n",
    "for datasetName in datasetNames:\n",
    "    print(\"Reading\",datasetName)\n",
    "    readDF(weightedConfig,unweightedConfig,datasetName)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7309da0f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Error Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bf5cf1bf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "education\n",
      "1109776\n",
      "1110531\n",
      "1108254\n",
      "------------------------------\n",
      "football\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leon.bornemann/dataset_versioning/finalExperiments/code/RoleMatchingEvaluation/plotting/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3072: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "689315\n",
      "478501\n",
      "1604\n",
      "------------------------------\n",
      "military\n",
      "124510\n",
      "182448\n",
      "63509\n",
      "------------------------------\n",
      "politics\n",
      "54416\n",
      "47745\n",
      "592\n",
      "------------------------------\n",
      "tv_and_film\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leon.bornemann/dataset_versioning/finalExperiments/code/RoleMatchingEvaluation/plotting/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3072: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "386086\n",
      "395445\n",
      "105\n",
      "------------------------------\n",
      "austintexas\n",
      "80808\n",
      "118932\n",
      "77977\n",
      "------------------------------\n",
      "chicago\n",
      "22890\n",
      "193796\n",
      "22105\n",
      "------------------------------\n",
      "gov.maryland\n",
      "2411\n",
      "53082\n",
      "1830\n",
      "------------------------------\n",
      "oregon\n",
      "37063\n",
      "37300\n",
      "36807\n",
      "------------------------------\n",
      "utah\n",
      "6269\n",
      "6695\n",
      "6223\n"
     ]
    }
   ],
   "source": [
    "for ds in [\"education\",\"football\",\"military\", \"politics\", \"tv_and_film\"]:\n",
    "    print(\"------------------------------\")\n",
    "    print(ds)\n",
    "    oldFootballDF = prepareDF(pd.read_csv(\"/san2/data/change-exploration/roleMerging/optimization/evaluation/\"+ds+\"/alpha_5.18E-4/edges.csv\"))\n",
    "    newFootballDF = allDfs[ds]['weighted']\n",
    "    edgesOld = set(oldFootballDF['edgeID'])\n",
    "    edgesNew = set(newFootballDF['edgeID'])\n",
    "    print(len(edgesOld))\n",
    "    print(len(edgesNew))\n",
    "    inBoth = edgesOld.intersection(edgesNew)\n",
    "    print(len(inBoth))\n",
    "    \n",
    "for ds in [\"austintexas\",\"chicago\", \"gov.maryland\",  \"oregon\",  \"utah\"]:\n",
    "    print(\"------------------------------\")\n",
    "    print(ds)\n",
    "    oldFootballDF = prepareDF(pd.read_csv(\"/san2/data/change-exploration/roleMerging/optimization/evaluation/\"+ds+\"/alpha_3.1E-5/edges.csv\"))\n",
    "    newFootballDF = allDfs[ds]['weighted']\n",
    "    edgesOld = set(oldFootballDF['edgeID'])\n",
    "    edgesNew = set(newFootballDF['edgeID'])\n",
    "    print(len(edgesOld))\n",
    "    print(len(edgesNew))\n",
    "    inBoth = edgesOld.intersection(edgesNew)\n",
    "    print(len(inBoth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0cc847e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "689315\n",
      "478501\n",
      "1604\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dd438d70",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [ComponentID, Method, cliqueID, cliqueSize, vertex1ID, vertex2ID, remainsValid, evidence, score, edgeID, edgeIDIsSorted]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "politics= allDfs['politics']['unweighted']\n",
    "print(politics[politics['cliqueID']==6773])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filled-conviction",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Precision Recall and F1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "presidential-avenue",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset,unweighted,unweighted,unweighted,weighted,weighted,weighted\n",
      ",precision,recall,f1,precision,recall,f1\n",
      "austintexas,0.83,0.22,0.35,0.7,0.64,0.67\n",
      "chicago,0.85,0.87,0.86,0.31,0.84,0.45\n",
      "gov.maryland,0.23,0.17,0.2,0.15,0.53,0.23\n",
      "oregon,0.57,0.34,0.43,0.35,0.86,0.5\n",
      "utah,0.91,0.98,0.94,0.89,0.95,0.92\n",
      "education,0.79,0.94,0.86,0.79,0.91,0.85\n",
      "football,0.68,0.57,0.62,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leon.bornemann/dataset_versioning/finalExperiments/code/RoleMatchingEvaluation/plotting/lib/python3.6/site-packages/ipykernel_launcher.py:20: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0,0.0,nan\n",
      "military,0.53,0.27,0.36,0.62,0.5,0.55\n",
      "politics,0.53,0.5,0.51,0.01,0.01,0.01\n",
      "tv_and_film,0.72,0.39,0.51,0.0,0.0,nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leon.bornemann/dataset_versioning/finalExperiments/code/RoleMatchingEvaluation/plotting/lib/python3.6/site-packages/ipykernel_launcher.py:20: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "def getPrecision(df,filterByInteresting):\n",
    "    dfToUse=df\n",
    "    if(filterByInteresting):\n",
    "        dfToUse=df[df['evidence']>0]\n",
    "    truth=dfToUse['remainsValid']\n",
    "    dfToUse['predicted']=True\n",
    "    predicted=dfToUse['predicted']\n",
    "    return accuracy_score(truth, predicted)\n",
    "\n",
    "def printSingleCSVString(dsName,configName,newline):\n",
    "    dfMaxRecall=allDfs[dsName]['maxRecall']\n",
    "    dfConfigResult=allDfs[dsName][configName]\n",
    "    dfConfigResult=dfConfigResult[dfConfigResult['evidence']>0]\n",
    "    validEdgesFromMaxRecall = dfMaxRecall[dfMaxRecall['remainsValid']]\n",
    "    validEdgesFromConfig = dfConfigResult[dfConfigResult['remainsValid']]\n",
    "    allPositives = set(validEdgesFromMaxRecall['edgeID'])\n",
    "    truePositives = set(validEdgesFromConfig['edgeID'])\n",
    "    recall = round(len(allPositives.intersection(truePositives)) / len(allPositives),2)\n",
    "    precision = round(getPrecision(dfConfigResult,True),2)\n",
    "    f1 = round(2*recall*precision/(recall+precision),2)\n",
    "    if(newline):\n",
    "        print(precision,recall,f1,sep=\",\")\n",
    "    else:\n",
    "        print(dsName,precision,recall,str(f1) + \",\",sep=\",\",end=\"\")\n",
    "\n",
    "def printCSVString(dsName,weightedConfigName,unweightedConfigName):\n",
    "    printSingleCSVString(dsName,unweightedConfigName,False)\n",
    "    printSingleCSVString(dsName,weightedConfigName,True)\n",
    "\n",
    "print(\"Dataset\",\"unweighted\",\"unweighted\",\"unweighted\",\"weighted\",\"weighted\",\"weighted\",sep=\",\")\n",
    "\n",
    "print(\"\",\"precision\",\"recall\",\"f1\",\"precision\",\"recall\",\"f1\",sep=\",\")\n",
    "\n",
    "for datasetName in datasetNames:\n",
    "    printCSVString(datasetName,weightedConfig,unweightedConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virtual-morning",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Exporting Role Matchings for Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ordinary-purchase",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Intersting failures (edges labelled as invalid sorted descendingly by score)\n",
    "\n",
    "\n",
    "###########################################################################\n",
    "#### REPLACE THIS WITH THE PATH TO YOUR Output Dir ########################\n",
    "exportPath = 'exportedData/' \n",
    "###########################################################################\n",
    "###########################################################################\n",
    "\n",
    "for dsName in datasetNamesWikipedia:\n",
    "    df = allDfs[dsName][weightedConfigWikipedia]\n",
    "    invalid = df[df['remainsValid']==False]\n",
    "    invalid = invalid.sort_values('score',ascending=False)\n",
    "    invalid.to_csv( exportPath + \"invalidEdges_\"+dsName+\".csv\")\n",
    "    \n",
    "for dsName in datasetNamesSocrata:\n",
    "    df = allDfs[dsName][weightedConfigSocrata]\n",
    "    invalid = df[df['remainsValid']==False]\n",
    "    invalid = invalid.sort_values('score',ascending=False)\n",
    "    invalid.to_csv(exportPath + \"invalidEdges_\"+dsName+\".csv\")    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "popular-universal",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#exporting max recall edge set:\n",
    "\n",
    "def exportEdgeIDs(configName,datasetName):\n",
    "    ds = allDfs[datasetName][configName]\n",
    "    path = exportPath + 'maxRecallEdges_' + datasetName + \".csv\"\n",
    "    ds['edgeID'].to_csv(path)\n",
    "\n",
    "for datasetName in datasetNamesSocrata:\n",
    "    exportEdgeIDs(unweightedConfigSocrata,datasetName) #does not matter which config we pass here, max recall is the same for both\n",
    "    \n",
    "for datasetName in datasetNamesWikipedia:\n",
    "    exportEdgeIDs(unweightedConfigWikipedia,datasetName)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "nasty-wound",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#exporting all true positive edges\n",
    "def exportTruePositiveEdgeIDs(configName,datasetName):\n",
    "    ds = allDfs[datasetName][configName]\n",
    "    ds = ds[ds['remainsValid']]\n",
    "    path = exportPath + 'truePositiveEdges_' + datasetName + \".csv\"\n",
    "    ds[['vertex1ID','vertex2ID']].to_csv(path)\n",
    "\n",
    "for datasetName in datasetNamesSocrata:\n",
    "    exportTruePositiveEdgeIDs(weightedConfigSocrata,datasetName)\n",
    "    \n",
    "for datasetName in datasetNamesWikipedia:\n",
    "    exportTruePositiveEdgeIDs(weightedConfigWikipedia,datasetName)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "major-electric",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Clique Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "enormous-bullet",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading austintexas\n",
      "Reading chicago\n",
      "Reading gov.maryland\n",
      "Reading oregon\n",
      "Reading utah\n",
      "Reading education\n",
      "Reading football\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leon.bornemann/dataset_versioning/finalExperiments/code/RoleMatchingEvaluation/plotting/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3263: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading military\n",
      "Reading politics\n",
      "Reading tv_and_film\n",
      "ComponentID                        object\n",
      "Method                             object\n",
      "cliqueID                            int64\n",
      "cliqueSize                          int64\n",
      "edgesTotal                          int64\n",
      "validEdges                          int64\n",
      "totalEvidence                       int64\n",
      "fractionOfVerticesWithEvidence    float64\n",
      "score                             float64\n",
      "alpha                             float64\n",
      "remainsValid                         bool\n",
      "dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leon.bornemann/dataset_versioning/finalExperiments/code/RoleMatchingEvaluation/plotting/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3263: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n"
     ]
    }
   ],
   "source": [
    "#header: ComponentID,Method,cliqueID,cliqueSize,edgesTotal,validEdges,totalEvidence,fractionOfVerticesWithEvidence,score,alpha\n",
    "\n",
    "def readCliqueDF(weightedConfig,unweightedConfig,datasetName):\n",
    "    pathWeighted=pathToData +datasetName + '/' + weightedConfig +'/cliques.csv'\n",
    "    weightedDF = pd.read_csv(pathWeighted)\n",
    "    weightedDF['remainsValid'] = (weightedDF['validEdges'] == weightedDF['edgesTotal'])\n",
    "    weightedDF = weightedDF[weightedDF['cliqueSize']>1]\n",
    "    \n",
    "    pathUnweighted=pathToData +datasetName + '/' + unweightedConfig +'/cliques.csv'\n",
    "    unweightedDF = pd.read_csv(pathUnweighted)\n",
    "    unweightedDF['remainsValid'] = (unweightedDF['validEdges'] == unweightedDF['edgesTotal'])\n",
    "    unweightedDF = unweightedDF[unweightedDF['cliqueSize']>1]\n",
    "    \n",
    "    datasetResult = {weightedConfig:weightedDF,unweightedConfig:unweightedDF}\n",
    "    allCliqueDfs[datasetName] = datasetResult\n",
    "\n",
    "weightedConfig = \"weighted\"\n",
    "unweightedConfig = \"unweighted\"\n",
    "datasetNames = [\"austintexas\",\"chicago\", \"gov.maryland\",  \"oregon\",  \"utah\", \"education\",\"football\",\"military\", \"politics\", \"tv_and_film\"]\n",
    "\n",
    "allCliqueDfs = {}\n",
    "\n",
    "for datasetName in datasetNames:\n",
    "    print(\"Reading\",datasetName)\n",
    "    readCliqueDF(weightedConfig,unweightedConfig,datasetName)\n",
    "    \n",
    "print(allCliqueDfs['chicago'][weightedConfig].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "seventh-spanish",
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------Weighted-----------------------------------------------\n",
      "Dataset,CliquePrecision\n",
      "austintexas,0.92\n",
      "chicago,0.92\n",
      "gov.maryland,0.26\n",
      "oregon,0.71\n",
      "utah,0.93\n",
      "education,0.36\n",
      "football,0.0\n",
      "military,0.29\n",
      "politics,0.01\n",
      "tv_and_film,0.0\n",
      "---------------------------------Unweighted-----------------------------------------------\n",
      "Dataset,CliquePrecision\n",
      "Dataset,CliquePrecision\n",
      "austintexas,0.85\n",
      "chicago,0.87\n",
      "gov.maryland,0.26\n",
      "oregon,0.67\n",
      "utah,0.91\n",
      "education,0.31\n",
      "football,0.36\n",
      "military,0.3\n",
      "politics,0.33\n",
      "tv_and_film,0.42\n"
     ]
    }
   ],
   "source": [
    "def printPrecision(dsName,configName):\n",
    "    df=allCliqueDfs[dsName][configName]\n",
    "    df=df[df['totalEvidence']>0]\n",
    "    validEdgesFromConfig = df[df['remainsValid']]\n",
    "    precision = len(validEdgesFromConfig.index) / len(df.index)\n",
    "    weightedPrecision = sum(validEdgesFromConfig['edgesTotal']) / sum(df['edgesTotal'])\n",
    "    print(dsName,round(precision,2),sep=\",\")\n",
    "    resultRow = {'dataset':dsName,'configName':configName,'Precision':precision}\n",
    "    resultDFRows.append(resultRow)\n",
    "\n",
    "resultDFRows=[]\n",
    "\n",
    "print(\"---------------------------------Weighted-----------------------------------------------\")\n",
    "print(\"Dataset\",\"CliquePrecision\",sep=\",\")\n",
    "for datasetName in datasetNames:\n",
    "    printPrecision(datasetName,weightedConfig)\n",
    "\n",
    "print(\"---------------------------------Unweighted-----------------------------------------------\")\n",
    "print(\"Dataset\",\"CliquePrecision\",sep=\",\")\n",
    "print(\"Dataset\",\"CliquePrecision\",sep=\",\")\n",
    "for datasetName in datasetNames:\n",
    "    printPrecision(datasetName,unweightedConfig)\n",
    "\n",
    "resultCliqueDF = pd.DataFrame(resultDFRows)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae58068d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03128255984735316"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(allCliqueDfs['football']['weighted']['remainsValid']) / len(allCliqueDfs['football']['weighted'].index) #59072,MDMCP,174607,2,1,1,0,0.0,2.4126667995005846E-5,5.18E-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96900d8e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}